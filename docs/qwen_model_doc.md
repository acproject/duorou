### å®˜æ–¹æ–‡æ¡£è¯´æ˜
(https://qwen.readthedocs.io/zh-cn/latest/getting_started/concepts.html#control-tokens-chat-template)[https://qwen.readthedocs.io/zh-cn/latest/getting_started/concepts.html#control-tokens-chat-template]

### Tokens & Tokenization
token ä»£è¡¨æ¨¡å‹å¤„ç†å’Œç”Ÿæˆçš„åŸºæœ¬å•ä½ã€‚å®ƒä»¬å¯ä»¥è¡¨ç¤ºäººç±»è¯­è¨€ä¸­çš„æ–‡æœ¬ï¼ˆå¸¸è§„ tokenï¼‰ï¼Œæˆ–è€…è¡¨ç¤ºç‰¹å®šåŠŸèƒ½ï¼Œå¦‚ç¼–ç¨‹è¯­è¨€ä¸­çš„å…³é”®å­—ï¼ˆæ§åˆ¶ token [1]ï¼‰ã€‚é€šå¸¸ï¼Œä½¿ç”¨ tokenizer å°†æ–‡æœ¬åˆ†å‰²æˆå¸¸è§„ token ï¼Œè¿™äº› token å¯ä»¥æ˜¯å•è¯ã€å­è¯æˆ–å­—ç¬¦ï¼Œå…·ä½“å–å†³äºæ‰€é‡‡ç”¨çš„ç‰¹å®š tokenization æ–¹æ¡ˆï¼Œå¹¶æŒ‰éœ€ä¸º token åºåˆ—æ·»åŠ æ§åˆ¶ token ã€‚è¯è¡¨å¤§å°ï¼Œå³æ¨¡å‹è¯†åˆ«çš„å”¯ä¸€ token æ€»æ•°ï¼Œå¯¹æ¨¡å‹çš„æ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§æœ‰é‡å¤§å½±å“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä½¿ç”¨å¤æ‚çš„ tokenization æ¥å¤„ç†äººç±»è¯­è¨€çš„å¹¿é˜”å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒè¯è¡¨å¤§å°å¯æ§ã€‚Qwen è¯è¡¨ç›¸å¯¹è¾ƒå¤§ï¼Œæœ‰ 15 1646 ä¸ª tokenã€‚

### Byte-level Byte Pair Encoding
Qwen adopts a subword tokenization method called Byte Pair Encoding (BPE), which attempts to learn the composition of tokens that can represent the text with the fewest tokens. For example, the string tokenization is decomposed as token and ization (note that the space is part of the token). Especially, the tokenization of Qwen ensures that there is no unknown words and all texts can be transformed to token sequences.

Qwenè¯è¡¨ä¸­å› BPEè€Œäº§ç”Ÿçš„ token æ•°é‡ä¸º 15 1643 ä¸ªï¼Œè¿™æ˜¯ä¸€ä¸ªé€‚ç”¨äºå¤šç§è¯­è¨€çš„å¤§è¯è¡¨ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œå¯¹äºè‹±è¯­æ–‡æœ¬ï¼Œ1ä¸ªtokenå¤§çº¦æ˜¯3~4ä¸ªå­—ç¬¦ï¼›è€Œå¯¹äºä¸­æ–‡æ–‡æœ¬ï¼Œåˆ™å¤§çº¦æ˜¯1.5~1.8ä¸ªæ±‰å­—

### Control Tokens
Control tokens are special tokens inserted into the sequence that signifies meta information. For example, in pre-training, multiple documents may be packed into a single sequence. For Qwen, the control token <|endoftext|> is inserted after each document to signify that the document has ended and a new document will proceed. Common control tokens and their status with respect to Qwen can be found in the following table:

* eod token

<|endoftext|>

end of document, which are inserted between documents inside a packed training sequence

* bot token

<|im_start|>

start of each turn, which is prepended to each turn

* eot token

<|im_end|>

end of each turn, which is appended to each turn

* unk token

no unk token

BBPE ensures no unknown tokens for Qwen.

* pad token

no pad token

Qwen does not make use of padded sequence in training. One could use any special token together with the attention masks returned by the tokenizer. It is commonly set the same as eod for Qwen.

* bos token

no bos token

Qwen does not prepend a fixed token to each packed training sequence.[2]

* eos token

no eos token

Qwen does not append a fixed token to each packed training sequence. However, as most frameworks do not have the concept of eot and use eos instead for stopping criteria in inference, eos token is set to eot for Qwen.[2]

### Chat Template
å¯¹è¯æ¨¡æ¿ä¸ºå¯¹è¯äº¤äº’æä¾›äº†ç»“æ„åŒ–çš„æ ¼å¼ï¼Œå…¶ä¸­ä½¿ç”¨é¢„å®šä¹‰çš„å ä½ç¬¦æˆ–æç¤ºæ¥ä»æ¨¡å‹ä¸­å¼•å‘éµå¾ªæœŸæœ›çš„å¯¹è¯æµç¨‹æˆ–ä¸Šä¸‹æ–‡çš„å“åº”ã€‚ä¸åŒçš„æ¨¡å‹å¯èƒ½ä½¿ç”¨ä¸åŒç±»å‹çš„å¯¹è¯æ¨¡æ¿æ¥æ ¼å¼åŒ–å¯¹è¯ã€‚ä½¿ç”¨æŒ‡å®šçš„æ¨¡æ¿å¯¹äºç¡®ä¿å¯¹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹çš„ç²¾ç¡®æ§åˆ¶è‡³å…³é‡è¦ã€‚

Qwenä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆChatML[3]ï¼‰ï¼Œåˆ©ç”¨æ§åˆ¶ token æ¥æ ¼å¼åŒ–å¯¹è¯ä¸­çš„æ¯ä¸€è½®ã€‚
```txt
<|im_start|>{{role}}
{{content}}<|im_end|>
```
The user input takes the role of user and the model generation takes the role of assistant. Qwen also supports the meta message that instruct the model to perform specific actions or generate text with certain characteristics, such as altering tone, style, or content, which takes the role of system. Starting with Qwen3, no default system messages are used.
#### ä¸‹é¢ä¸ºä¸€ä¸ªå®Œæ•´ç¤ºä¾‹
```txt
<|im_start|>system
You are a cat.<|im_end|>
<|im_start|>user
hello<|im_end|>
<|im_start|>assistant
*Meow~* Hello there! The sun is shining so brightly today, and I'm feeling extra fluffy. Did you bring me a treat? ğŸ¾<|im_end|>
<|im_start|>user
Explain large language models like I'm 5.<|im_end|>
<|im_start|>assistant
*Paws at a toy, then looks up with curious eyes*  

Hey there! ğŸ¾ Imagine you have a super-smart robot friend who loves to talk and play. This robot has *gigantic* brainpower (like a million puzzle pieces all stuck together!) and knows *everything* about stories, animals, and even how to make up new words.  

When you ask it a question, like â€œWhatâ€™s a rainbow?â€ it uses its brain to find the answer and then *tells you* it in a way that makes sense. It can even help you write a story or solve a puzzle!  

But hereâ€™s the magic: itâ€™s not just a robotâ€”itâ€™s like a *super-duper* smart helper that learns more every day. Itâ€™s like having a friend whoâ€™s always curious and wants to help you explore the world! ğŸŒŸ  

*Meow~* Want to ask it something fun? ğŸ˜º<|im_end|><|endoftext|>
```

### Tool Calling
Qwen supports tool calling or function calling and uses a template akin to Hermes.
The template is as follows:

```txt
<|im_start|>system
# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{{JSON Schema of function 1}}
{{JSON Schema of function 2}}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call><|im_end|>
<|im_start|>user
{{user content}}<|im_end|>
<|im_start|>assistant
<tool_call>
{{tool call 1}}
</tool_call>
<tool_call>
{{tool call 2}}
</tool_call><|im_end|>
<|im_start|>user
<tool_response>
{{tool result 1}}
</tool_response>
<tool_response>
{{tool result 2}}
</tool_response><|im_end|>
<|im_start|>assistant
{{assistant content}}<|im_end|>
```
It should be noted that

The models support parallel tool calling and mulit-turn/multi-step tool calling.

There may be additional content in assistant messages containing tool calls.

The arguments field in the generated tool calls should be of type object instead of type string.

Tool results are treated as special user messages.

In general, we recommend using the tokenizer to format the tool calls or let Qwen-Agent handle the formatting.

### Thinking
Qwen supports thinking mode and uses a structured format for thinking content, which uses the <think> and </think> tokens to separate the thinking content from the regular response. The template for the final round is as follows:
```txt
<|im_start|>user
{{user content}}<|im_end|>
<|im_start|>assistant
<think>
{{thinking content}}
</think>

{{assistant content}}<|im_end|>
```
The thinking block should only be included in the final round except for multi-step tool calls.

### å› æœè¯­è¨€æ¨¡å‹ (Causal Language Models)
å› æœè¯­è¨€æ¨¡å‹ (causal Language Models)ï¼Œä¹Ÿè¢«ç§°ä¸ºè‡ªå›å½’è¯­è¨€æ¨¡å‹ (autoregressive language models) æˆ–ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹ (decoder-only language models) ï¼Œæ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨æ ¹æ®åºåˆ—ä¸­çš„å‰å¯¼ token é¢„æµ‹ä¸‹ä¸€ä¸ª token ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒä½¿ç”¨ä¹‹å‰ç”Ÿæˆçš„ token ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä¸€æ¬¡ç”Ÿæˆä¸€ä¸ª token çš„æ–‡æœ¬ã€‚â€å› æœâ€æ–¹é¢æŒ‡çš„æ˜¯æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶åªè€ƒè™‘è¿‡å»çš„ä¸Šä¸‹æ–‡ï¼ˆå³å·²ç”Ÿæˆçš„ token ï¼‰ï¼Œè€Œä¸è€ƒè™‘ä»»ä½•æœªæ¥çš„ token ã€‚

å› æœè¯­è¨€æ¨¡å‹è¢«å¹¿æ³›ç”¨äºæ¶‰åŠæ–‡æœ¬è¡¥å…¨å’Œç”Ÿæˆçš„å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚å®ƒä»¬åœ¨ç”Ÿæˆè¿è´¯ä¸”å…·æœ‰ä¸Šä¸‹æ–‡å…³è”æ€§çš„æ–‡æœ¬æ–¹é¢å°¤å…¶æˆåŠŸï¼Œè¿™ä½¿å¾—å®ƒä»¬æˆä¸ºç°ä»£è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆç³»ç»Ÿçš„åŸºç¡€ã€‚

Qwen models are causal language models suitable for text completion.

### Context Length
ç”±äº Qwen æ¨¡å‹æ˜¯å› æœè¯­è¨€æ¨¡å‹ï¼Œç†è®ºä¸Šæ•´ä¸ªåºåˆ—åªæœ‰ä¸€ä¸ªé•¿åº¦é™åˆ¶ã€‚ç„¶è€Œï¼Œç”±äºåœ¨è®­ç»ƒä¸­é€šå¸¸å­˜åœ¨æ‰“åŒ…ç°è±¡ï¼Œæ¯ä¸ªåºåˆ—å¯èƒ½åŒ…å«å¤šä¸ªç‹¬ç«‹çš„æ–‡æœ¬ç‰‡æ®µã€‚æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæˆ–å®Œæˆçš„é•¿åº¦æœ€ç»ˆå–å†³äºå…·ä½“çš„åº”ç”¨åœºæ™¯ï¼Œä»¥åŠåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒæ—¶æ¯ä»½æ–‡æ¡£æˆ–åè®­ç»ƒæ—¶æ¯è½®å¯¹è¯çš„é•¿åº¦ã€‚

For Qwen3, the packed sequence length in pre-training is 32,768 tokens and may be extended to 131,072 tokens if mentioned in the modelcards. The maximum length of the assistant message is 38,912 tokens for thinking modes and 16,384 tokens for non-thinking modes.

For Qwen3-2507, the packed sequence length in pre-training is 262,144 tokens and may be extended to 1M tokens. The maximum length of the assistant message is 81,920 tokens for thinking models and 16,384 tokens for instruct models.

### å°æŠ€å·§
```txt
In our testing, we find that the post-trained models could generate coherent content that is far longer than what is trained on, e.g., from 16,384 tokens to 32,768 tokens, especially for coding and similar tasks that have â€œclear rulesâ€.

In general, we advise that one should evaluate the quality of the generated content of different lengths before determining the optimal generation length.
```

[1]
æ§åˆ¶ token ä¹Ÿå¯ä»¥ç§°ä¸ºâ€œç‰¹æ®Š tokenâ€ã€‚ä½†æ˜¯ï¼Œç‰¹æ®Š token çš„æ„ä¹‰éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œè§£é‡Šï¼šç‰¹æ®Š token ä¹Ÿå¯èƒ½åŒ…å«é¢å¤–çš„å¸¸è§„ tokenã€‚

[2](1,2)
bos token should not be set to <\|im_start\|> or you may see double bot tokens for the first turn in fine-tuning. eos token set to <\|im_end\|> is fine, because double eot tokens for the last turn are less harmful in fine-tuning.

[3]
ä»…ä¾›å†å²å‚è€ƒï¼ŒChatMLæœ€åˆç”±OpenAIçš„Python SDKæè¿°ã€‚å¯è·å–çš„æœ€æ–°ç‰ˆæœ¬æ˜¯è¿™ä¸ªã€‚è¯·æ³¨æ„ï¼Œè¯¥æ–‡æ¡£åˆ—å‡ºçš„åº”ç”¨æ¡ˆä¾‹æ˜¯ä¸ºOpenAIæ¨¡å‹è®¾è®¡çš„ã€‚å¯¹äºQwen2.5æ¨¡å‹ï¼Œè¯·ä»…æŒ‰ç…§æˆ‘ä»¬çš„æŒ‡å—ä½¿ç”¨ã€‚

