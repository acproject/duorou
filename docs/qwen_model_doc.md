### å®˜æ–¹æ–‡æ¡£è¯´æ˜
(https://qwen.readthedocs.io/zh-cn/latest/getting_started/concepts.html#control-tokens-chat-template)[https://qwen.readthedocs.io/zh-cn/latest/getting_started/concepts.html#control-tokens-chat-template]

### Tokens & Tokenization
token ä»£è¡¨æ¨¡å‹å¤„ç†å’Œç”Ÿæˆçš„åŸºæœ¬å•ä½ã€‚å®ƒä»¬å¯ä»¥è¡¨ç¤ºäººç±»è¯­è¨€ä¸­çš„æ–‡æœ¬ï¼ˆå¸¸è§„ tokenï¼‰ï¼Œæˆ–è€…è¡¨ç¤ºç‰¹å®šåŠŸèƒ½ï¼Œå¦‚ç¼–ç¨‹è¯­è¨€ä¸­çš„å…³é”®å­—ï¼ˆæ§åˆ¶ token [1]ï¼‰ã€‚é€šå¸¸ï¼Œä½¿ç”¨ tokenizer å°†æ–‡æœ¬åˆ†å‰²æˆå¸¸è§„ token ï¼Œè¿™äº› token å¯ä»¥æ˜¯å•è¯ã€å­è¯æˆ–å­—ç¬¦ï¼Œå…·ä½“å–å†³äºæ‰€é‡‡ç”¨çš„ç‰¹å®š tokenization æ–¹æ¡ˆï¼Œå¹¶æŒ‰éœ€ä¸º token åºåˆ—æ·»åŠ æ§åˆ¶ token ã€‚è¯è¡¨å¤§å°ï¼Œå³æ¨¡å‹è¯†åˆ«çš„å”¯ä¸€ token æ€»æ•°ï¼Œå¯¹æ¨¡å‹çš„æ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§æœ‰é‡å¤§å½±å“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä½¿ç”¨å¤æ‚çš„ tokenization æ¥å¤„ç†äººç±»è¯­è¨€çš„å¹¿é˜”å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒè¯è¡¨å¤§å°å¯æ§ã€‚Qwen è¯è¡¨ç›¸å¯¹è¾ƒå¤§ï¼Œæœ‰ 15 1646 ä¸ª tokenã€‚

### Byte-level Byte Pair Encoding
Qwen adopts a subword tokenization method called Byte Pair Encoding (BPE), which attempts to learn the composition of tokens that can represent the text with the fewest tokens. For example, the string tokenization is decomposed as token and ization (note that the space is part of the token). Especially, the tokenization of Qwen ensures that there is no unknown words and all texts can be transformed to token sequences.

Qwenè¯è¡¨ä¸­å› BPEè€Œäº§ç”Ÿçš„ token æ•°é‡ä¸º 15 1643 ä¸ªï¼Œè¿™æ˜¯ä¸€ä¸ªé€‚ç”¨äºå¤šç§è¯­è¨€çš„å¤§è¯è¡¨ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œå¯¹äºè‹±è¯­æ–‡æœ¬ï¼Œ1ä¸ªtokenå¤§çº¦æ˜¯3~4ä¸ªå­—ç¬¦ï¼›è€Œå¯¹äºä¸­æ–‡æ–‡æœ¬ï¼Œåˆ™å¤§çº¦æ˜¯1.5~1.8ä¸ªæ±‰å­—

### Control Tokens
Control tokens are special tokens inserted into the sequence that signifies meta information. For example, in pre-training, multiple documents may be packed into a single sequence. For Qwen, the control token <|endoftext|> is inserted after each document to signify that the document has ended and a new document will proceed. Common control tokens and their status with respect to Qwen can be found in the following table:

* eod token

<|endoftext|>

end of document, which are inserted between documents inside a packed training sequence

* bot token

<|im_start|>

start of each turn, which is prepended to each turn

* eot token

<|im_end|>

end of each turn, which is appended to each turn

* unk token

no unk token

BBPE ensures no unknown tokens for Qwen.

* pad token

no pad token

Qwen does not make use of padded sequence in training. One could use any special token together with the attention masks returned by the tokenizer. It is commonly set the same as eod for Qwen.

* bos token

no bos token

Qwen does not prepend a fixed token to each packed training sequence.[2]

* eos token

no eos token

Qwen does not append a fixed token to each packed training sequence. However, as most frameworks do not have the concept of eot and use eos instead for stopping criteria in inference, eos token is set to eot for Qwen.[2]

### Chat Template
å¯¹è¯æ¨¡æ¿ä¸ºå¯¹è¯äº¤äº’æä¾›äº†ç»“æ„åŒ–çš„æ ¼å¼ï¼Œå…¶ä¸­ä½¿ç”¨é¢„å®šä¹‰çš„å ä½ç¬¦æˆ–æç¤ºæ¥ä»æ¨¡å‹ä¸­å¼•å‘éµå¾ªæœŸæœ›çš„å¯¹è¯æµç¨‹æˆ–ä¸Šä¸‹æ–‡çš„å“åº”ã€‚ä¸åŒçš„æ¨¡å‹å¯èƒ½ä½¿ç”¨ä¸åŒç±»å‹çš„å¯¹è¯æ¨¡æ¿æ¥æ ¼å¼åŒ–å¯¹è¯ã€‚ä½¿ç”¨æŒ‡å®šçš„æ¨¡æ¿å¯¹äºç¡®ä¿å¯¹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹çš„ç²¾ç¡®æ§åˆ¶è‡³å…³é‡è¦ã€‚

Qwenä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆChatML[3]ï¼‰ï¼Œåˆ©ç”¨æ§åˆ¶ token æ¥æ ¼å¼åŒ–å¯¹è¯ä¸­çš„æ¯ä¸€è½®ã€‚
```txt
<|im_start|>{{role}}
{{content}}<|im_end|>
```
The user input takes the role of user and the model generation takes the role of assistant. Qwen also supports the meta message that instruct the model to perform specific actions or generate text with certain characteristics, such as altering tone, style, or content, which takes the role of system. Starting with Qwen3, no default system messages are used.
#### ä¸‹é¢ä¸ºä¸€ä¸ªå®Œæ•´ç¤ºä¾‹
```txt
<|im_start|>system
You are a cat.<|im_end|>
<|im_start|>user
hello<|im_end|>
<|im_start|>assistant
*Meow~* Hello there! The sun is shining so brightly today, and I'm feeling extra fluffy. Did you bring me a treat? ğŸ¾<|im_end|>
<|im_start|>user
Explain large language models like I'm 5.<|im_end|>
<|im_start|>assistant
*Paws at a toy, then looks up with curious eyes*  

Hey there! ğŸ¾ Imagine you have a super-smart robot friend who loves to talk and play. This robot has *gigantic* brainpower (like a million puzzle pieces all stuck together!) and knows *everything* about stories, animals, and even how to make up new words.  

When you ask it a question, like â€œWhatâ€™s a rainbow?â€ it uses its brain to find the answer and then *tells you* it in a way that makes sense. It can even help you write a story or solve a puzzle!  

But hereâ€™s the magic: itâ€™s not just a robotâ€”itâ€™s like a *super-duper* smart helper that learns more every day. Itâ€™s like having a friend whoâ€™s always curious and wants to help you explore the world! ğŸŒŸ  

*Meow~* Want to ask it something fun? ğŸ˜º<|im_end|><|endoftext|>
```

### Tool Calling
Qwen supports tool calling or function calling and uses a template akin to Hermes.
The template is as follows:

```txt
<|im_start|>system
# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{{JSON Schema of function 1}}
{{JSON Schema of function 2}}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call><|im_end|>
<|im_start|>user
{{user content}}<|im_end|>
<|im_start|>assistant
<tool_call>
{{tool call 1}}
</tool_call>
<tool_call>
{{tool call 2}}
</tool_call><|im_end|>
<|im_start|>user
<tool_response>
{{tool result 1}}
</tool_response>
<tool_response>
{{tool result 2}}
</tool_response><|im_end|>
<|im_start|>assistant
{{assistant content}}<|im_end|>
```
It should be noted that

The models support parallel tool calling and mulit-turn/multi-step tool calling.

There may be additional content in assistant messages containing tool calls.

The arguments field in the generated tool calls should be of type object instead of type string.

Tool results are treated as special user messages.

In general, we recommend using the tokenizer to format the tool calls or let Qwen-Agent handle the formatting.

### Thinking
Qwen supports thinking mode and uses a structured format for thinking content, which uses the <think> and </think> tokens to separate the thinking content from the regular response. The template for the final round is as follows:
```txt
<|im_start|>user
{{user content}}<|im_end|>
<|im_start|>assistant
<think>
{{thinking content}}
</think>

{{assistant content}}<|im_end|>
```
The thinking block should only be included in the final round except for multi-step tool calls.

### å› æœè¯­è¨€æ¨¡å‹ (Causal Language Models)
å› æœè¯­è¨€æ¨¡å‹ (causal Language Models)ï¼Œä¹Ÿè¢«ç§°ä¸ºè‡ªå›å½’è¯­è¨€æ¨¡å‹ (autoregressive language models) æˆ–ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹ (decoder-only language models) ï¼Œæ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨æ ¹æ®åºåˆ—ä¸­çš„å‰å¯¼ token é¢„æµ‹ä¸‹ä¸€ä¸ª token ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒä½¿ç”¨ä¹‹å‰ç”Ÿæˆçš„ token ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä¸€æ¬¡ç”Ÿæˆä¸€ä¸ª token çš„æ–‡æœ¬ã€‚â€å› æœâ€æ–¹é¢æŒ‡çš„æ˜¯æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶åªè€ƒè™‘è¿‡å»çš„ä¸Šä¸‹æ–‡ï¼ˆå³å·²ç”Ÿæˆçš„ token ï¼‰ï¼Œè€Œä¸è€ƒè™‘ä»»ä½•æœªæ¥çš„ token ã€‚

å› æœè¯­è¨€æ¨¡å‹è¢«å¹¿æ³›ç”¨äºæ¶‰åŠæ–‡æœ¬è¡¥å…¨å’Œç”Ÿæˆçš„å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚å®ƒä»¬åœ¨ç”Ÿæˆè¿è´¯ä¸”å…·æœ‰ä¸Šä¸‹æ–‡å…³è”æ€§çš„æ–‡æœ¬æ–¹é¢å°¤å…¶æˆåŠŸï¼Œè¿™ä½¿å¾—å®ƒä»¬æˆä¸ºç°ä»£è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆç³»ç»Ÿçš„åŸºç¡€ã€‚

Qwen models are causal language models suitable for text completion.

### Context Length
ç”±äº Qwen æ¨¡å‹æ˜¯å› æœè¯­è¨€æ¨¡å‹ï¼Œç†è®ºä¸Šæ•´ä¸ªåºåˆ—åªæœ‰ä¸€ä¸ªé•¿åº¦é™åˆ¶ã€‚ç„¶è€Œï¼Œç”±äºåœ¨è®­ç»ƒä¸­é€šå¸¸å­˜åœ¨æ‰“åŒ…ç°è±¡ï¼Œæ¯ä¸ªåºåˆ—å¯èƒ½åŒ…å«å¤šä¸ªç‹¬ç«‹çš„æ–‡æœ¬ç‰‡æ®µã€‚æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæˆ–å®Œæˆçš„é•¿åº¦æœ€ç»ˆå–å†³äºå…·ä½“çš„åº”ç”¨åœºæ™¯ï¼Œä»¥åŠåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒæ—¶æ¯ä»½æ–‡æ¡£æˆ–åè®­ç»ƒæ—¶æ¯è½®å¯¹è¯çš„é•¿åº¦ã€‚

For Qwen3, the packed sequence length in pre-training is 32,768 tokens and may be extended to 131,072 tokens if mentioned in the modelcards. The maximum length of the assistant message is 38,912 tokens for thinking modes and 16,384 tokens for non-thinking modes.

For Qwen3-2507, the packed sequence length in pre-training is 262,144 tokens and may be extended to 1M tokens. The maximum length of the assistant message is 81,920 tokens for thinking models and 16,384 tokens for instruct models.

### å°æŠ€å·§
```txt
In our testing, we find that the post-trained models could generate coherent content that is far longer than what is trained on, e.g., from 16,384 tokens to 32,768 tokens, especially for coding and similar tasks that have â€œclear rulesâ€.

In general, we advise that one should evaluate the quality of the generated content of different lengths before determining the optimal generation length.
```

[1]
æ§åˆ¶ token ä¹Ÿå¯ä»¥ç§°ä¸ºâ€œç‰¹æ®Š tokenâ€ã€‚ä½†æ˜¯ï¼Œç‰¹æ®Š token çš„æ„ä¹‰éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œè§£é‡Šï¼šç‰¹æ®Š token ä¹Ÿå¯èƒ½åŒ…å«é¢å¤–çš„å¸¸è§„ tokenã€‚

[2](1,2)
bos token should not be set to <\|im_start\|> or you may see double bot tokens for the first turn in fine-tuning. eos token set to <\|im_end\|> is fine, because double eot tokens for the last turn are less harmful in fine-tuning.

[3]
ä»…ä¾›å†å²å‚è€ƒï¼ŒChatMLæœ€åˆç”±OpenAIçš„Python SDKæè¿°ã€‚å¯è·å–çš„æœ€æ–°ç‰ˆæœ¬æ˜¯è¿™ä¸ªã€‚è¯·æ³¨æ„ï¼Œè¯¥æ–‡æ¡£åˆ—å‡ºçš„åº”ç”¨æ¡ˆä¾‹æ˜¯ä¸ºOpenAIæ¨¡å‹è®¾è®¡çš„ã€‚å¯¹äºQwen2.5æ¨¡å‹ï¼Œè¯·ä»…æŒ‰ç…§æˆ‘ä»¬çš„æŒ‡å—ä½¿ç”¨ã€‚

### 28-layer Transformer æ¨ç†æµç¨‹å›¾ï¼ˆä¸è¯´æ˜ï¼‰

```mermaid
flowchart TD
  A[Input tokens (t0..tN)] --> B[Token Embedding + Positional Encoding]
  B --> C{Transformer Layers 1..28}
  subgraph LAYERS [28 x Transformer Layer (sequential)]
    direction TB
    L1[Layer 1]
    L2[Layer 2]
    L3[Layer 3]
    L4[Layer 4]
    L5[Layer 5]
    L6[Layer 6]
    L7[Layer 7]
    L8[Layer 8]
    L9[Layer 9]
    L10[Layer 10]
    L11[Layer 11]
    L12[Layer 12]
    L13[Layer 13]
    L14[Layer 14]
    L15[Layer 15]
    L16[Layer 16]
    L17[Layer 17]
    L18[Layer 18]
    L19[Layer 19]
    L20[Layer 20]
    L21[Layer 21]
    L22[Layer 22]
    L23[Layer 23]
    L24[Layer 24]
    L25[Layer 25]
    L26[Layer 26]
    L27[Layer 27]
    L28[Layer 28]
  end
  C --> D[Output head (LM head / softmax)]
  D --> E[Sample / Argmax â†’ Next token]
  E --> F[Append to context (kv-cache used for efficiency)]
  F --> C

  classDef layerbox fill:#f8f9fa,stroke:#333,stroke-width:1px;
  class L1,L2,L3,L4,L5,L6,L7,L8,L9,L10,L11,L12,L13,L14,L15,L16,L17,L18,L19,L20,L21,L22,L23,L24,L25,L26,L27,L28 layerbox;
```

**è¯´æ˜ï¼ˆç®€è¦ï¼‰**
- æ¯ä¸€å±‚ï¼ˆLayerï¼‰å†…éƒ¨å…¸å‹ç»“æ„ï¼š
  1. Multi-head Self-Attentionï¼ˆå¸¦ causal maskï¼Œç”¨äºè‡ªå›å½’ç”Ÿæˆï¼‰
  2. Add & LayerNorm
  3. Feed-Forward Networkï¼ˆé€šå¸¸æ˜¯ä¸¤å±‚å¸¦æ¿€æ´»ï¼Œå¦‚SwiGLUï¼‰
  4. Add & LayerNorm
- æ¨ç†æ—¶éœ€è¦*é¡ºåºæ‰§è¡Œ*æ‰€æœ‰å±‚ï¼ˆç¬¬1å±‚çš„è¾“å‡ºä½œä¸ºç¬¬2å±‚è¾“å…¥ï¼‰ï¼Œç›´åˆ°ç¬¬28å±‚åè¾“å‡ºåˆ°è¯­è¨€å»ºæ¨¡å¤´ï¼ˆsoftmaxï¼‰å¾—åˆ°ä¸‹ä¸€ä¸ª token çš„åˆ†å¸ƒã€‚
- ä¸ºäº†åŠ é€Ÿè‡ªå›å½’æ¨ç†ï¼Œä¼šä¿å­˜å„å±‚çš„ Key/Valueï¼ˆkv-cacheï¼‰ï¼Œä¸‹æ¬¡ç”Ÿæˆ token æ—¶åªéœ€è¦è®¡ç®—æ–°çš„ Query ä¸ç¼“å­˜çš„ KV çš„ Attentionï¼Œé¿å…é‡å¤è®¡ç®—å†å²ä¸Šä¸‹æ–‡ã€‚

---

### åŸºäº Qwen2.5-VL çš„å±‚æ¬¡å›¾ä¸æ¨ç†æµç¨‹ï¼ˆæ¦‚è§ˆï¼‰

```mermaid
flowchart TD
  subgraph VIS [è§†è§‰å­ç½‘: ViT (Window Attention)]
    VIn[Image Input] --> Patch[Patchify + Linear Embed]
    Patch --> WinAtt[Window Attention Blocks]
    WinAtt --> VFeat[Visual Feature Tokens]
  end

  subgraph TXT [æ–‡æœ¬å­ç½‘: LLM Backbone (stack of Transformer layers)]
    TIn[Text tokens] --> TEmb[Token Embedding + MRoPE]
    TEmb --> LStack[LLM Transformer Layers (e.g. 28/xx/..)]
    LStack --> LOut[Language Modeling Head]
  end

  VFeat --> Fuse[Projection & Multimodal Fusion]
  Fuse --> TEmb
  LOut --> Decode[Softmax -> Sampling]

  note right of Fuse
    Fusion can be: prepend visual tokens to text tokens,
    or cross-attention / multimodal adapter.
  end
```

**è¯´æ˜ï¼ˆç®€è¦ï¼‰**
- Qwen2.5-VL çš„å¸¸è§æ¨¡å—ï¼šè§†è§‰ç¼–ç å™¨ï¼ˆViTï¼Œå¸¦ window attention ä¼˜åŒ–ï¼‰ã€ä¸€ç»„æŠ•å½±å±‚ï¼ˆæŠŠè§†è§‰ token æ˜ å°„åˆ° LLM çš„ embedding ç©ºé—´ï¼‰ã€ä»¥åŠåŸºäº Qwen2.5 çš„ LLM backboneï¼ˆè‹¥å¹²å±‚ transformerï¼‰ã€‚
- å¯¹äºè§†é¢‘ï¼ŒQwen2.5-VL å¼•å…¥ dynamic FPS sampling ä»¥è‡ªé€‚åº”æ—¶ç©ºé‡‡æ ·ï¼›å¯¹ä½ç½®ä¿¡æ¯é‡‡ç”¨å‡çº§ç‰ˆçš„ MRoPEï¼ˆmulti-resolutional rotary positional encodingï¼‰ã€‚

---

# æ¨ç†æµç¨‹ï¼ˆé€æ­¥ï¼‰

1. **è¾“å…¥å‡†å¤‡**
   - æ–‡æœ¬ï¼šTokenizeï¼ˆBPE / SentencePieceï¼‰ï¼Œå¾—åˆ° token idsã€‚
   - å›¾åƒï¼šPatchify â†’ linear projection â†’ å¾—åˆ° visual tokensï¼ˆæˆ–ç”¨ CNN/ViT æå–æ± åŒ–ç‰¹å¾ï¼‰ã€‚
   - è‹¥åŒæ—¶è¾“å…¥å›¾åƒä¸æ–‡æœ¬ï¼Œä¼šå°†è§†è§‰ token æŠ•å½±åˆ°ä¸æ–‡æœ¬ embedding ç›¸åŒç»´åº¦ï¼Œä½œä¸ºé¢å¤–çš„åºåˆ—ç‰‡æ®µï¼ˆæˆ–é€šè¿‡ cross-attention æ³¨å…¥ï¼‰ã€‚

2. **Embedding + Positional Encoding**
   - æ–‡æœ¬ embedding + MRoPE / RotaryPosï¼›è§†è§‰ token é€šå¸¸å¸¦æœ‰ç©ºé—´/æ—¶é—´ä½ç½®ç¼–ç ï¼ˆwindow-attention å†…éƒ¨æˆ–å¤–éƒ¨ï¼‰ã€‚

3. **Transformer Backbone å‰å‘**
   - å¯¹äºæ¯ä¸€ä¸ªè‡ªå›å½’ç”Ÿæˆæ­¥éª¤ï¼š
     - å¦‚æœå¯ç”¨ kv-cacheï¼šä»…ä¸ºæ–°ç”Ÿæˆ token è®¡ç®— Query å¹¶ä¸ç¼“å­˜ KV åš Attentionï¼ŒèŠ‚çœè®¡ç®—ã€‚
     - æ¯å±‚æ‰§è¡Œï¼šAttention â†’ Add&Norm â†’ FFN (SwiGLU) â†’ Add&Normã€‚

4. **è¾“å‡ºå¤´ä¸è§£ç **
   - æœ€åä¸€å±‚è¾“å‡ºæŠ•å½±åˆ°è¯è¡¨ logits â†’ softmax å¾—åˆ°æ¦‚ç‡ã€‚
   - é‡‡æ ·ç­–ç•¥ï¼šGreedy / Top-k / Top-p (nucleus) / Temperature è°ƒæ•´ï¼›æˆ–ä½¿ç”¨ Beam Searchï¼ˆå¯¹è¯æˆ–å¼ºçº¦æŸåœºæ™¯ï¼‰ã€‚

5. **å¤šæ¨¡æ€ç‰¹æœ‰æ­¥éª¤**
   - è§†è§‰ç†è§£é˜¶æ®µå¯èƒ½åŒ…å«ï¼šwindow attentionï¼ˆåŠ é€Ÿ ViTï¼‰ã€object detection head æˆ– OCR å­æ¨¡å—ã€bounding-box å›å½’ã€‚
   - è§†é¢‘ï¼šåŠ¨æ€ FPS é‡‡æ · â†’ æ—¶é—´è½´ç‰¹å¾èšåˆã€‚

---

# æ¨ç†ä¸­ç”¨åˆ°çš„ä¸»è¦ç®—æ³•ä¸ä¼˜åŒ–æŠ€æœ¯ï¼ˆæ¸…å•ï¼‰

- **ç¼–ç /åˆ†è¯**ï¼šBPE / SentencePieceï¼›Tokenizer truncation & padding
- **ä½ç½®ç¼–ç **ï¼šRoPE / MRoPEï¼ˆmulti-resolution rotary positional encï¼‰
- **Attention**ï¼šScaled dot-product attentionï¼›Multi-head attentionï¼›Causal masking
- **é«˜æ•ˆ Attention å®ç°**ï¼šFlashAttentionã€memory-efficient attentionã€sparse/windowed attentionï¼ˆViT çš„ window attentionï¼‰
- **FFN æ¿€æ´»**ï¼šGELUã€SwiGLUï¼ˆQwen ç³»åˆ—å¸¸ç”¨ï¼‰
- **æ­£åˆ™åŒ–/å½’ä¸€åŒ–**ï¼šRMSNorm / LayerNormï¼ˆQwen2.5 æ–‡æ¡£æåˆ° RMSNorm å¯¹ ViT å¯¹é½æœ‰å¸®åŠ©ï¼‰
- **å¤šæ¨¡æ€èåˆ**ï¼šè§†è§‰ token æŠ•prepend / cross-attention / multimodal adapters
- **KV-cache**ï¼šè‡ªå›å½’æ¨ç†ä¸­ä¿å­˜æ¯ä¸€å±‚ K/Vï¼Œé¿å…é‡å¤è®¡ç®—å†å²ä¸Šä¸‹æ–‡
- **é‡‡æ ·/è§£ç **ï¼šGreedy, Beam Search, Top-k, Top-p (nucleus sampling), Temperature
- **å·¥ç¨‹çº§åˆ«ä¼˜åŒ–**ï¼š
  - Mixed Precisionï¼ˆfp16 / bf16ï¼‰åŠ é€Ÿ
  - æƒé‡é‡åŒ–ï¼ˆ8-bit, 4-bitï¼‰
  - Tensor Parallelism / Pipeline Parallelism / Model Sharding
  - Operator fusionï¼ˆattention + softmax ä¼˜åŒ–ï¼‰
  - ç¼–è¯‘å™¨ä¼˜åŒ–ï¼ˆTensorRT, Tritonã€XLAï¼‰

---

# é™„ï¼šå¸¸è§çš„å·¥ç¨‹è½åœ°æ³¨æ„ç‚¹

- å¦‚æœä½ è¦åœ¨ GPU ä¸Šåšä½å»¶è¿Ÿæ¨ç†ï¼š
  - ä½¿ç”¨ kv-cache + FlashAttention + mixed precision
  - ç”¨ 4-bit/8-bit é‡åŒ–å‡å°æ˜¾å­˜å ç”¨ï¼ˆéœ€éªŒè¯ç²¾åº¦ï¼‰
- å¦‚æœæ˜¯å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾+æ–‡ï¼‰ï¼šæå‰æ‰¹å¤„ç†è§†è§‰ encoderï¼Œç¼“å­˜è§†è§‰ç‰¹å¾ï¼Œå†ä¸æ–‡æœ¬èµ° LLM backbone
- åœ¨åš early-exit åŠ¨æ€å±‚æ•°ä¼˜åŒ–æ—¶ï¼Œéœ€æƒè¡¡å‡†ç¡®ç‡ä¸å»¶è¿Ÿï¼ˆå¤šæ•° SOTA LLM ä¸é»˜è®¤å¼€å¯ early-exitï¼‰


---
### ollama manifests
```json
{
    "schemaVersion": 2,
    "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
    "config": {
        "mediaType": "application/vnd.docker.container.image.v1+json",
        "digest": "sha256:83b9da835d9f13632a97e550cc8fd02ff7f39b88a843f0a8923330890682977a",
        "size": 567
    },
    "layers": [
        {
            "mediaType": "application/vnd.ollama.image.model",
            "digest": "sha256:a99b7f834d754b88f122d865f32758ba9f0994a83f8363df2c1e71c17605a025",
            "size": 5969233408
        },
        {
            "mediaType": "application/vnd.ollama.image.template",
            "digest": "sha256:a242d8dfdc8f8c2b0586ee85fba70adb408fb633aba2836fe1b05f2c46631474",
            "size": 487
        },
        {
            "mediaType": "application/vnd.ollama.image.system",
            "digest": "sha256:75357d685f238b6afd7738be9786fdafde641eb6ca9a3be7471939715a68a4de",
            "size": 28
        },
        {
            "mediaType": "application/vnd.ollama.image.license",
            "digest": "sha256:832dd9e00a68dd83b3c3fb9f5588dad7dcf337a0db50f7d9483f310cd292e92e",
            "size": 11343
        },
        {
            "mediaType": "application/vnd.ollama.image.params",
            "digest": "sha256:52d2a7aa3a380c606bd1cd3d6f777a9c65a1c77c2e0cb091eed2968a5ef04dc3",
            "size": 23
        }
    ]
}
```